{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "072c5367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/saitosatoru/miniconda3/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: tqdm in /Users/saitosatoru/miniconda3/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: pandas in /Users/saitosatoru/miniconda3/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: scapy in /Users/saitosatoru/miniconda3/lib/python3.12/site-packages (2.6.1)\n",
      "Requirement already satisfied: matplotlib in /Users/saitosatoru/miniconda3/lib/python3.12/site-packages (3.10.3)\n",
      "Requirement already satisfied: urllib3 in /Users/saitosatoru/miniconda3/lib/python3.12/site-packages (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/saitosatoru/miniconda3/lib/python3.12/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/saitosatoru/miniconda3/lib/python3.12/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/saitosatoru/miniconda3/lib/python3.12/site-packages (from requests) (2025.4.26)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/saitosatoru/miniconda3/lib/python3.12/site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/saitosatoru/miniconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/saitosatoru/miniconda3/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/saitosatoru/miniconda3/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/saitosatoru/miniconda3/lib/python3.12/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/saitosatoru/miniconda3/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/saitosatoru/miniconda3/lib/python3.12/site-packages (from matplotlib) (4.58.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/saitosatoru/miniconda3/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/saitosatoru/miniconda3/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /Users/saitosatoru/miniconda3/lib/python3.12/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/saitosatoru/miniconda3/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/saitosatoru/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests tqdm pandas scapy matplotlib urllib3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639f4190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  - Date Range: 2025-04-09 to 2025-04-09\n",
      "  - PCAP Dir:   /Users/saitosatoru/workspace/GDELT_EDA/data/MAWI_ditl/pcap\n",
      "  - CSV Dir:    /Users/saitosatoru/workspace/GDELT_EDA/data/MAWI_ditl/csv\n",
      "  - Cleanup Files: True\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Daily processing:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Daily processing:   0%|          | 0/1 [00:00<?, ?it/s]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipping existing 202504090000.pcap.gz...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "mawi_daily_pcap_stats.py (Jupyter Notebook / tshark Fast Version)\n",
    "────────────────────────────────────────────────────────\n",
    "【高速版】tshark を利用してMAWI DITLのpcapファイルを高速に処理するスクリプト。\n",
    "15分単位のpcap.gzをダウンロードし、1時間ごとにプロトコル別、\n",
    "ユニークIP/ポート数の統計情報を集計し、CSVに保存後、可視化します。\n",
    "\n",
    "【主なフロー】\n",
    "1) 指定日付範囲をループ。\n",
    "2) 1日あたり96個の15分pcapをダウンロード・展開。\n",
    "3) ★★★ tshark を使ってpcapから必要な情報を高速に抽出 ★★★\n",
    "4) 処理後、pcapファイルとgzファイルはすぐに削除。\n",
    "5) 1日分のデータをpandasで高速に集計し、<YYYY-MM-DD>.csvに追記。\n",
    "6) 全CSVを連結して、4段構成のグラフを描画。\n",
    "\n",
    "【重要】\n",
    "このスクリプトを実行するには、tshark (Wireshark) が\n",
    "お使いの環境にインストールされている必要があります。\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import gzip\n",
    "import shutil\n",
    "import socket\n",
    "import subprocess\n",
    "import io\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, date, time, timedelta, timezone\n",
    "from pathlib import Path\n",
    "from typing import Dict, Set\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from matplotlib import dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "from requests.adapters import HTTPAdapter\n",
    "from tqdm import tqdm\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "\n",
    "JST = timezone(timedelta(hours=9), 'JST')\n",
    "UTC = timezone.utc\n",
    "\n",
    "\n",
    "def build_session(retries: int = 3, backoff: float = 1.0) -> requests.Session:\n",
    "    retry_strategy = Retry(\n",
    "        total=retries,\n",
    "        backoff_factor=backoff,\n",
    "        status_forcelist=(500, 502, 503, 504),\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    sess = requests.Session()\n",
    "    sess.mount(\"https://\", adapter)\n",
    "    sess.mount(\"http://\", adapter)\n",
    "    return sess\n",
    "\n",
    "\n",
    "\n",
    "def fetch_and_unzip_segment(target_dt_utc: datetime, dest_dir: Path) -> Path | None:\n",
    "    year = target_dt_utc.year\n",
    "    stamp = target_dt_utc.strftime(\"%Y%m%d%H%M\")\n",
    "    url = f\"https://mawi.wide.ad.jp/mawi/ditl/ditl{year}/{stamp}.pcap.gz\"\n",
    "    \n",
    "    day_dir = dest_dir / f\"{target_dt_utc:%Y%m%d}\"\n",
    "    day_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    gz_path = day_dir / f\"{stamp}.pcap.gz\"\n",
    "    pcap_path = gz_path.with_suffix(\".pcap\")\n",
    "\n",
    "    if pcap_path.exists():\n",
    "        return pcap_path\n",
    "\n",
    "    if gz_path.exists():\n",
    "        tqdm.write(f\"Unzipping existing {gz_path.name}...\")\n",
    "        try:\n",
    "            with gzip.open(gz_path, \"rb\") as fin, open(pcap_path, \"wb\") as fout:\n",
    "                shutil.copyfileobj(fin, fout)\n",
    "            return pcap_path\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"[ERROR] Gunzip failed for {gz_path.name}: {e}\")\n",
    "            return None\n",
    "\n",
    "    sess = build_session()\n",
    "    try:\n",
    "        with sess.get(url, stream=True, timeout=300) as resp:\n",
    "            if resp.status_code == 404:\n",
    "                return None\n",
    "            resp.raise_for_status()\n",
    "            total = int(resp.headers.get(\"Content-Length\", 0))\n",
    "            with open(gz_path, \"wb\") as fh, tqdm(\n",
    "                total=total, unit=\"B\", unit_scale=True, desc=f\"DL {gz_path.name}\", leave=False\n",
    "            ) as bar:\n",
    "                for chunk in resp.iter_content(1 << 20):\n",
    "                    fh.write(chunk)\n",
    "                    bar.update(len(chunk))\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        tqdm.write(f\"[ERROR] Download failed for {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        with gzip.open(gz_path, \"rb\") as fin, open(pcap_path, \"wb\") as fout:\n",
    "            shutil.copyfileobj(fin, fout)\n",
    "        return pcap_path\n",
    "    except Exception as e:\n",
    "        tqdm.write(f\"[ERROR] Gunzip failed for {gz_path.name}: {e}\")\n",
    "        gz_path.unlink(missing_ok=True)\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_stats_df_from_pcap_with_tshark(pcap_path: Path) -> pd.DataFrame | None:\n",
    "    \"\"\"tshark を使用してpcapファイルから統計情報を抽出し、pandas DataFrameとして返す\"\"\"\n",
    "    tshark_fields = [\n",
    "        \"-e\", \"frame.time_epoch\",      # タイムスタンプ (Epoch)\n",
    "        \"-e\", \"ip.src\",                # 送信元IP\n",
    "        \"-e\", \"ip.dst\",                # 宛先IP\n",
    "        \"-e\", \"_ws.col.Protocol\",      # プロトコル名 (簡易)\n",
    "        \"-e\", \"ip.proto\",              # プロトコル番号\n",
    "        \"-e\", \"tcp.srcport\",           # TCP送信元ポート\n",
    "        \"-e\", \"tcp.dstport\",           # TCP宛先ポート\n",
    "        \"-e\", \"udp.srcport\",           # UDP送信元ポート\n",
    "        \"-e\", \"udp.dstport\",           # UDP宛先ポート\n",
    "        \"-e\", \"frame.len\",             # フレーム長\n",
    "    ]\n",
    "    \n",
    "    command = [\n",
    "        \"tshark\", \"-r\", str(pcap_path), \"-T\", \"fields\", \"-E\", \"separator=,\", \"-E\", \"quote=d\",\n",
    "        *tshark_fields\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(command, capture_output=True, text=True, check=True)\n",
    "    except FileNotFoundError:\n",
    "        tqdm.write(\"[FATAL ERROR] `tshark` command not found. Please install Wireshark/tshark and ensure it's in your system's PATH.\")\n",
    "        return None\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        tqdm.write(f\"[ERROR] tshark failed to process {pcap_path.name}: {e.stderr}\")\n",
    "        return None\n",
    "\n",
    "    col_names = [\"time_epoch\", \"src_ip\", \"dst_ip\", \"protocol_name\", \"protocol_num\",\n",
    "                 \"tcp_src_port\", \"tcp_dst_port\", \"udp_src_port\", \"udp_dst_port\", \"length\"]\n",
    "    \n",
    "    df = pd.read_csv(io.StringIO(result.stdout), header=None, names=col_names, low_memory=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_date_range(start: date, end: date, csv_dir: Path, pcap_dir: Path, cleanup_pcap: bool):\n",
    "    \"\"\"指定日付範囲のデータを処理し、CSVに追記。pcapは逐次削除。\"\"\"\n",
    "    csv_dir.mkdir(parents=True, exist_ok=True)\n",
    "    now_utc = datetime.now(UTC)\n",
    "\n",
    "    days = [start + timedelta(days=i) for i in range((end - start).days + 1)]\n",
    "    for current_d in tqdm(days, desc=\"Daily processing\"):\n",
    "        day_prefix = f\"[{current_d:%Y-%m-%d}]\"\n",
    "        \n",
    "        daily_dfs = []\n",
    "\n",
    "        total_segments = 24 * 4\n",
    "        segment_iter = tqdm(range(total_segments), desc=f\"Segments for {current_d:%Y-%m-%d}\", leave=False)\n",
    "        for i in segment_iter:\n",
    "            h, m = divmod(i * 15, 60)\n",
    "            target_dt_utc = datetime.combine(current_d, time(h, m), tzinfo=UTC)\n",
    "\n",
    "            if target_dt_utc >= now_utc: continue\n",
    "\n",
    "            pcap_path = fetch_and_unzip_segment(target_dt_utc, pcap_dir)\n",
    "            if not pcap_path: continue\n",
    "            \n",
    "            segment_df = get_stats_df_from_pcap_with_tshark(pcap_path)\n",
    "            if segment_df is not None and not segment_df.empty:\n",
    "                daily_dfs.append(segment_df)\n",
    "\n",
    "            if cleanup_pcap and pcap_path:\n",
    "                gz_path = pcap_path.with_suffix(\".pcap.gz\")\n",
    "                pcap_path.unlink(missing_ok=True)\n",
    "                gz_path.unlink(missing_ok=True)\n",
    "        \n",
    "        if not daily_dfs:\n",
    "            tqdm.write(f\"{day_prefix} No data found for this day. Skipping CSV write.\")\n",
    "            continue\n",
    "            \n",
    "        full_day_df = pd.concat(daily_dfs, ignore_index=True)\n",
    "        tqdm.write(f\"{day_prefix} Aggregating {len(full_day_df)} total packets...\")\n",
    "\n",
    "        full_day_df['time_epoch'] = pd.to_numeric(full_day_df['time_epoch'], errors='coerce')\n",
    "        full_day_df['length'] = pd.to_numeric(full_day_df['length'], errors='coerce')\n",
    "        full_day_df.dropna(subset=['time_epoch', 'length'], inplace=True)\n",
    "        \n",
    "        full_day_df['timestamp_utc'] = pd.to_datetime(full_day_df['time_epoch'], unit='s', utc=True)\n",
    "        full_day_df['hour_utc'] = full_day_df['timestamp_utc'].dt.floor('H')\n",
    "\n",
    "        def classify_protocol(row):\n",
    "            if row['protocol_num'] == 1: return 'icmp'\n",
    "            if row['protocol_num'] == 6: return 'tcp'\n",
    "            if row['protocol_num'] == 17: return 'udp'\n",
    "            return 'other'\n",
    "        full_day_df['protocol'] = full_day_df.apply(classify_protocol, axis=1)\n",
    "\n",
    "        agg_funcs = {\n",
    "            'length': 'sum',\n",
    "            'time_epoch': 'count',\n",
    "            'src_ip': pd.Series.nunique,\n",
    "            'dst_ip': pd.Series.nunique,\n",
    "        }\n",
    "        \n",
    "        hourly_agg = full_day_df.groupby('hour_utc').agg(agg_funcs)\n",
    "        hourly_agg.rename(columns={'length': 'total_bytes', 'time_epoch': 'total_packets',\n",
    "                                   'src_ip': 'unique_src_ips', 'dst_ip': 'unique_dst_ips'}, inplace=True)\n",
    "        \n",
    "        proto_agg = full_day_df.groupby(['hour_utc', 'protocol']).agg({'length': 'sum', 'time_epoch': 'count'}).unstack(fill_value=0)\n",
    "        proto_agg.columns = [f'{p}_{m}' for m, p in proto_agg.columns]\n",
    "        \n",
    "        src_ports = full_day_df.groupby('hour_utc')['tcp_src_port'].nunique() + full_day_df.groupby('hour_utc')['udp_src_port'].nunique()\n",
    "        dst_ports = full_day_df.groupby('hour_utc')['tcp_dst_port'].nunique() + full_day_df.groupby('hour_utc')['udp_dst_port'].nunique()\n",
    "        port_agg = pd.DataFrame({'unique_src_ports': src_ports, 'unique_dst_ports': dst_ports})\n",
    "\n",
    "        final_df = hourly_agg.join(proto_agg).join(port_agg).reset_index()\n",
    "        final_df['timestamp_jst'] = final_df['hour_utc'].dt.tz_convert(JST).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        csv_path = csv_dir / f\"{current_d:%Y-%m-%d}.csv\"\n",
    "        final_cols = [\n",
    "            \"timestamp_jst\", \"total_bytes\", \"total_packets\",\n",
    "            \"tcp_bytes\", \"tcp_packets\", \"udp_bytes\", \"udp_packets\",\n",
    "            \"icmp_bytes\", \"icmp_packets\", \"other_bytes\", \"other_packets\",\n",
    "            \"unique_src_ips\", \"unique_dst_ips\", \"unique_src_ports\", \"unique_dst_ports\"\n",
    "        ]\n",
    "        final_df.rename(columns={'length_tcp': 'tcp_bytes', 'time_epoch_tcp': 'tcp_packets', \n",
    "                                 'length_udp': 'udp_bytes', 'time_epoch_udp': 'udp_packets',\n",
    "                                 'length_icmp': 'icmp_bytes', 'time_epoch_icmp': 'icmp_packets',\n",
    "                                 'length_other': 'other_bytes', 'time_epoch_other': 'other_packets'}, inplace=True)\n",
    "                                 \n",
    "        for col in final_cols:\n",
    "            if col not in final_df.columns:\n",
    "                final_df[col] = 0\n",
    "                \n",
    "        final_df[final_cols].to_csv(csv_path, index=False)\n",
    "        tqdm.write(f\"{day_prefix} Write complete.\")\n",
    "\n",
    "\n",
    "def plot_stats(csv_dir: Path, start_date: date, end_date: date):\n",
    "    \"\"\"CSVを読み込み、4段構成のグラフを描画・保存する\"\"\"\n",
    "    all_files = sorted(csv_dir.glob(\"*.csv\"))\n",
    "    if not all_files:\n",
    "        print(\"[INFO] No CSV files found; nothing to plot.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df = pd.concat((pd.read_csv(f, parse_dates=[\"timestamp_jst\"]) for f in all_files), ignore_index=True)\n",
    "        df[\"timestamp\"] = df[\"timestamp_jst\"].dt.tz_localize(JST)\n",
    "        df = df.dropna(subset=[\"timestamp\"]).drop_duplicates(\"timestamp\", keep=\"last\").set_index(\"timestamp\").sort_index()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to load or process CSVs: {e}\")\n",
    "        return\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"[INFO] No data available to plot.\")\n",
    "        return\n",
    "\n",
    "    fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(16, 24), sharex=True)\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    protocols = [\"tcp\", \"udp\", \"icmp\", \"other\"]\n",
    "    colors = {\"tcp\": \"#1f77b4\", \"udp\": \"#2ca02c\", \"icmp\": \"#d62728\", \"other\": \"#9467bd\"}\n",
    "    \n",
    "    for col_family in [\"bytes\", \"packets\"]:\n",
    "        for p in protocols:\n",
    "            col = f\"{p}_{col_family}\"\n",
    "            if col not in df.columns:\n",
    "                df[col] = 0\n",
    "\n",
    "    byte_cols = [f\"{p}_bytes\" for p in protocols]\n",
    "    df_bytes_gb = df[byte_cols].div(1e9)\n",
    "    ax1.stackplot(df.index, df_bytes_gb.T, labels=[p.upper() for p in protocols], colors=[colors[p] for p in protocols])\n",
    "    ax1.set_ylabel(\"Traffic Volume (GB)\")\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "    ax1.set_title(\"Hourly Traffic Volume by Protocol\")\n",
    "    ax1.grid(True)\n",
    "\n",
    "    packet_cols = [f\"{p}_packets\" for p in protocols]\n",
    "    df_packets_mil = df[packet_cols].div(1e6)\n",
    "    ax2.stackplot(df.index, df_packets_mil.T, labels=[p.upper() for p in protocols], colors=[colors[p] for p in protocols])\n",
    "    ax2.set_ylabel(\"Packets (Millions)\")\n",
    "    ax2.legend(loc=\"upper left\")\n",
    "    ax2.set_title(\"Hourly Packet Count by Protocol\")\n",
    "    ax2.grid(True)\n",
    "\n",
    "    ax3.plot(df.index, df[\"unique_src_ips\"], label=\"Unique Source IPs\", color=\"cyan\")\n",
    "    ax3.plot(df.index, df[\"unique_dst_ips\"], label=\"Unique Destination IPs\", color=\"magenta\", linestyle='--')\n",
    "    ax3.set_ylabel(\"Unique IP Count\")\n",
    "    ax3.legend(loc=\"upper left\")\n",
    "    ax3.set_title(\"Hourly Unique IP Address Count\")\n",
    "    ax3.grid(True)\n",
    "    \n",
    "    ax4.plot(df.index, df[\"unique_src_ports\"], label=\"Unique Source Ports\", color=\"orange\")\n",
    "    ax4.plot(df.index, df[\"unique_dst_ports\"], label=\"Unique Destination Ports\", color=\"purple\", linestyle='--')\n",
    "    ax4.set_ylabel(\"Unique Port Count\")\n",
    "    ax4.legend(loc=\"upper left\")\n",
    "    ax4.set_title(\"Hourly Unique Port Number Count\")\n",
    "    ax4.grid(True)\n",
    "\n",
    "    ax4.set_xlabel(\"Timestamp (JST)\")\n",
    "    ax4.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d\\n%H:%M\", tz=JST))\n",
    "    fig.autofmt_xdate(rotation=0, ha=\"center\")\n",
    "\n",
    "    title_start = df.index.min().date()\n",
    "    title_end = df.index.max().date()\n",
    "    fig.suptitle(f\"MAWI Hourly Traffic Analysis ({title_start:%Y-%m-%d} to {title_end:%Y-%m-%d})\", fontsize=16)\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    \n",
    "    plot_filename = csv_dir.parent / f\"mawi_stats_tshark_{start_date:%Y%m%d}-{end_date:%Y%m%d}.png\"\n",
    "    plt.savefig(plot_filename)\n",
    "    print(f\"\\nPlot saved to {plot_filename.resolve()}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "START_DATE = date(2025, 4, 9)\n",
    "END_DATE   = date(2025, 4, 9)\n",
    "\n",
    "PCAP_DIR = Path(\"./data/MAWI_ditl/pcap\")\n",
    "CSV_DIR  = Path(\"./data/MAWI_ditl/csv\")\n",
    "\n",
    "CLEANUP_FILES = True\n",
    "\n",
    "\n",
    "if START_DATE > END_DATE:\n",
    "    print(\"[ERROR] Start date cannot be after end date.\")\n",
    "else:\n",
    "    print(\"Configuration:\")\n",
    "    print(f\"  - Date Range: {START_DATE} to {END_DATE}\")\n",
    "    print(f\"  - PCAP Dir:   {PCAP_DIR.resolve()}\")\n",
    "    print(f\"  - CSV Dir:    {CSV_DIR.resolve()}\")\n",
    "    print(f\"  - Cleanup Files: {CLEANUP_FILES}\\n\")\n",
    "\n",
    "    process_date_range(\n",
    "        start=START_DATE, \n",
    "        end=END_DATE,\n",
    "        csv_dir=CSV_DIR, \n",
    "        pcap_dir=PCAP_DIR,\n",
    "        cleanup_pcap=CLEANUP_FILES\n",
    "    )\n",
    "\n",
    "    print(\"\\nPlotting results...\")\n",
    "    plot_stats(CSV_DIR, START_DATE, END_DATE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33bbf8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/saitosatoru/miniconda3/lib/python3.12/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/saitosatoru/miniconda3/lib/python3.12/site-packages (from ipywidgets) (9.3.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/saitosatoru/miniconda3/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: decorator in /Users/saitosatoru/miniconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /Users/saitosatoru/miniconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/saitosatoru/miniconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/saitosatoru/miniconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/saitosatoru/miniconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /Users/saitosatoru/miniconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/saitosatoru/miniconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack_data in /Users/saitosatoru/miniconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/saitosatoru/miniconda3/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/saitosatoru/miniconda3/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/saitosatoru/miniconda3/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/saitosatoru/miniconda3/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/saitosatoru/miniconda3/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /Users/saitosatoru/miniconda3/lib/python3.12/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Downloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "Downloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.7 jupyterlab_widgets-3.0.15 widgetsnbextension-4.0.14\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
